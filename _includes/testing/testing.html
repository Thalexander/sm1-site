<section id="testing" class="download">
    <div class="container">
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h2 class="text-center"><b>Testing</b></h2>
                <hr>
                <br>
                <h3 class="text-center"><b>Testing Methods</b></h3>
                <p>To validate that no existing code had been broken as we iterated on code during development, we wrote test suites to programmatically test different components of the bot.</p>
                <p>To make sure that we could provide a great user experience for consumers, we also performed user acceptance testing to obtain feedback on the bot's design and functionality.</p>
                <br>
                <p class="text-center"><b>Unit Testing</b></p>
                <p>Unit testing allows the developer to independently test small “units” of the project using unit test functions. Using the open source Mocha test framework for Node.js, we wrote test suites to test conversational functionality as well as commands (/help) and product searches. To programmatically test conversations with the bot, we also leveraged the ConsoleConnector in the Microsoft Bot Framework to send bot ouptut to the terminal where it could be read and parsed against expected results.</p>
                <br>
                <p class="text-center"><b>Integration Testing</b></p>
                <p>Integration testing tests the communication between units of the code, in order to ensure that all units work while interacting with each other. Our automated test suite encompasses the flow of conversation between the user and the bot from conversation initiation to a request for product recommendations - this enables us to test end-to-end integration of the bot to validate that it is working as expected without requiring manual verification.</p>
                <br>
                <p class="text-center"><b>Acceptance Testing</b></p>
                <p>To ensure a good user experience, we performed user acceptance testing (UAT) with regular users and the client to monitor whether we are meeting the requirements that the client has set out. Our tests with users were done in groups of 5. We focused on testing the natural language models that we have implemented as well as the conversational flow for interaction with the chatbot to ensure that it aligned with HCI principles. We received positive feedback during the testing - the results were promising. We found that at times we needed to improve the language models for a better user interaction experience and made appropriate changes.</p>
                <p><b>Acceptance Test Details:</b></p>
                <p><b>LUIS Model:</b> During development we developed a number of LUIS models with different entities and intents. We requested 5 users to test the language understanding abilities of the best 2 LUIS models we made and trained. We used a simple item entity model and another model that used IAB item classification, which divided items into categories while assigning them to entities. During our test process, ⅘ users reported that from their 10 requests for specific items, they got more responses when using the simple LUIS model. This made us realise that training a model with more entities, requires adding more utterances for each entity than the number of utterances we added for the simple model.</p>
                <p><b>Chatbot flow of control:</b> To understand if we have fulfilled the HCI principles of the bot, we asked another set of 5 users that had never used the bot before, to try to learn about the bot as much as they can in 2 minutes and fill in a checklist of functionalities they have found out. from these 5 people, 1 of them could find all of the functionalities, 3 of them could figure out 80% of the functionalities, and one of them 60%. Which in overall gives us a good rate of understanding in regards to HCI principles.</p>
                <hr>
            </div>
        </div>
    </div>
</section>
